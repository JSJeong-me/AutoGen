{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNg0mttA+kxNsd9WGxirtdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AutoGen/blob/main/ver049/AgentChat02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jWtQtmpGJ2w",
        "outputId": "6b779696-28b9-47e5-b8d6-0fceafda0636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-agentchat\n",
            "  Downloading autogen_agentchat-0.4.9.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting autogen-ext[openai]\n",
            "  Downloading autogen_ext-0.4.9.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting autogen-core==0.4.9.2 (from autogen-agentchat)\n",
            "  Downloading autogen_core-0.4.9.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core==0.4.9.2->autogen-agentchat)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.9.2->autogen-agentchat) (1.31.0)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.9.2->autogen-agentchat) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.9.2->autogen-agentchat) (5.29.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.9.2->autogen-agentchat) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.4.9.2->autogen-agentchat) (4.12.2)\n",
            "Collecting aiofiles (from autogen-ext[openai])\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: openai>=1.52.2 in /usr/local/lib/python3.11/dist-packages (from autogen-ext[openai]) (1.66.3)\n",
            "Collecting tiktoken>=0.8.0 (from autogen-ext[openai])\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.2->autogen-ext[openai]) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.2->autogen-ext[openai]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.2->autogen-ext[openai]) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.2->autogen-ext[openai]) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.2->autogen-ext[openai]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.2->autogen-ext[openai]) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->autogen-ext[openai]) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.52.2->autogen-ext[openai]) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[openai]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[openai]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.2->autogen-ext[openai]) (0.14.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.9.2->autogen-agentchat) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.9.2->autogen-agentchat) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.9.2->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.9.2->autogen-agentchat) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.8.0->autogen-ext[openai]) (2.3.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.4.9.2->autogen-agentchat) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.4.9.2->autogen-agentchat) (3.21.0)\n",
            "Downloading autogen_agentchat-0.4.9.2-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogen_core-0.4.9.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading autogen_ext-0.4.9.2-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.2/234.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: jsonref, aiofiles, tiktoken, autogen-core, autogen-ext, autogen-agentchat\n",
            "Successfully installed aiofiles-24.1.0 autogen-agentchat-0.4.9.2 autogen-core-0.4.9.2 autogen-ext-0.4.9.2 jsonref-1.1.0 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"autogen-agentchat\" \"autogen-ext[openai]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key  = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "FvIlE3bBGrbc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# Define a model client. You can use other model client that implements\n",
        "# the `ChatCompletionClient` interface.\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"gpt-4o\",\n",
        "    # api_key=\"YOUR_API_KEY\",\n",
        ")\n",
        "\n",
        "\n",
        "# Define a simple function tool that the agent can use.\n",
        "# For this example, we use a fake weather tool for demonstration purposes.\n",
        "async def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the weather for a given city.\"\"\"\n",
        "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
        "\n",
        "\n",
        "# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n",
        "# The system message instructs the agent via natural language.\n",
        "agent = AssistantAgent(\n",
        "    name=\"weather_agent\",\n",
        "    model_client=model_client,\n",
        "    tools=[get_weather],\n",
        "    system_message=\"You are a helpful assistant.\",\n",
        "    reflect_on_tool_use=True,\n",
        "    model_client_stream=True,  # Enable streaming tokens from the model client.\n",
        ")\n",
        "\n",
        "\n",
        "# Run the agent and stream the messages to the console.\n",
        "async def main() -> None:\n",
        "    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n",
        "\n",
        "\n",
        "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiUkB2SYGo4d",
        "outputId": "57e74b96-2500-46de-b560-e04c51913934"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "What is the weather in New York?\n",
            "---------- weather_agent ----------\n",
            "[FunctionCall(id='call_u9h4Rzj918e1prrK8zgIIQeR', arguments='{\"city\":\"New York\"}', name='get_weather')]\n",
            "---------- weather_agent ----------\n",
            "[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', name='get_weather', call_id='call_u9h4Rzj918e1prrK8zgIIQeR', is_error=False)]\n",
            "---------- weather_agent ----------\n",
            "The current weather in New York is 73 degrees and sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "async def main() -> None:\n",
        "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n",
        "\n",
        "    assistant = AssistantAgent(\n",
        "        name=\"assistant\",\n",
        "        system_message=\"You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.\",\n",
        "        model_client=model_client,\n",
        "    )\n",
        "\n",
        "    code_executor = CodeExecutorAgent(\n",
        "        name=\"code_executor\",\n",
        "        code_executor=LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
        "    )\n",
        "\n",
        "    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.\n",
        "    termination = TextMentionTermination(\"TERMINATE\") | MaxMessageTermination(10)\n",
        "\n",
        "    # The group chat will alternate between the assistant and the code executor.\n",
        "    group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)\n",
        "\n",
        "    # `run_stream` returns an async generator to stream the intermediate messages.\n",
        "    stream = group_chat.run_stream(task=\"Write a python script to print 'Hello, world!'\")\n",
        "    # `Console` is a simple UI to display the stream.\n",
        "    await Console(stream)\n",
        "\n",
        "# asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "GK3Uex0nIaQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYSYXDbqIoTJ",
        "outputId": "006761ef-ddab-4f57-8f40-8cf41b1ef253"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Write a python script to print 'Hello, world!'\n",
            "---------- assistant ----------\n",
            "```python\n",
            "print('Hello, world!')\n",
            "```\n",
            "---------- code_executor ----------\n",
            "Hello, world!\n",
            "\n",
            "---------- assistant ----------\n",
            "TERMINATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install autogen-agentchat"
      ],
      "metadata": {
        "id": "oQMUymqmJggi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install autogen"
      ],
      "metadata": {
        "id": "WQnkgNjtJuT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function\n",
        "\n",
        "# llm_config = {\n",
        "#     \"config_list\": [{\"model\": \"gpt-4o\", \"api_key\": \"sk-xxx\"}],\n",
        "#     \"seed\": 42,\n",
        "#     \"temperature\": 0,\n",
        "# }\n",
        "\n",
        "# tool_caller = AssistantAgent(\n",
        "#     name=\"tool_caller\",\n",
        "#     system_message=\"You are a helpful assistant. You can call tools to help user.\",\n",
        "#     llm_config=llm_config,\n",
        "#     max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot.\n",
        "# )\n",
        "\n",
        "# tool_executor = UserProxyAgent(\n",
        "#     name=\"tool_executor\",\n",
        "#     human_input_mode=\"NEVER\",\n",
        "#     code_execution_config=False,\n",
        "#     llm_config=False,\n",
        "# )\n",
        "\n",
        "# def get_weather(city: str) -> str:\n",
        "#     return f\"The weather in {city} is 72 degree and sunny.\"\n",
        "\n",
        "# # Register the tool function to the tool caller and executor.\n",
        "# register_function(get_weather, caller=tool_caller, executor=tool_executor)\n",
        "\n",
        "# while True:\n",
        "#     user_input = input(\"User: \")\n",
        "#     if user_input == \"exit\":\n",
        "#         break\n",
        "#     chat_result = tool_executor.initiate_chat(\n",
        "#         tool_caller,\n",
        "#         message=user_input,\n",
        "#         summary_method=\"reflection_with_llm\", # To let the model reflect on the tool use, set to \"last_msg\" to return the tool call result directly.\n",
        "#     )\n",
        "#     print(\"Assistant:\", chat_result.summary)\n"
      ],
      "metadata": {
        "id": "17l3ohi1JXtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from autogen_core import CancellationToken\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.messages import TextMessage\n",
        "\n",
        "def get_weather(city: str) -> str: # Async tool is possible too.\n",
        "    return f\"The weather in {city} is 72 degree and sunny.\"\n",
        "\n",
        "async def main() -> None:\n",
        "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n",
        "    assistant = AssistantAgent(\n",
        "        name=\"assistant\",\n",
        "        system_message=\"You are a helpful assistant. You can call tools to help user.\",\n",
        "        model_client=model_client,\n",
        "        tools=[get_weather],\n",
        "        reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.\n",
        "    )\n",
        "    while True:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input == \"exit\":\n",
        "            break\n",
        "        response = await assistant.on_messages([TextMessage(content=user_input, source=\"user\")], CancellationToken())\n",
        "        print(\"Assistant:\", response.chat_message.content)\n",
        "\n",
        "# asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "qfhxpwFiKZ6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsjBHFNfKfJE",
        "outputId": "b76f8856-e456-4de8-b157-5c365097cf68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Seoul\n",
            "Assistant: The current weather in Seoul is 72°F and sunny.\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.conditions import TextMentionTermination\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "async def main() -> None:\n",
        "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", seed=42, temperature=0)\n",
        "\n",
        "    writer = AssistantAgent(\n",
        "        name=\"writer\",\n",
        "        description=\"A writer.\",\n",
        "        system_message=\"You are a writer.\",\n",
        "        model_client=model_client,\n",
        "    )\n",
        "\n",
        "    critic = AssistantAgent(\n",
        "        name=\"critic\",\n",
        "        description=\"A critic.\",\n",
        "        system_message=\"You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.\",\n",
        "        model_client=model_client,\n",
        "    )\n",
        "\n",
        "    # The termination condition is a text termination, which will cause the chat to terminate when the text \"APPROVE\" is received.\n",
        "    termination = TextMentionTermination(\"APPROVE\")\n",
        "\n",
        "    # The group chat will alternate between the writer and the critic.\n",
        "    group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)\n",
        "\n",
        "    # `run_stream` returns an async generator to stream the intermediate messages.\n",
        "    stream = group_chat.run_stream(task=\"Write a short story about a robot that discovers it has feelings.\")\n",
        "    # `Console` is a simple UI to display the stream.\n",
        "    await Console(stream)\n",
        "\n",
        "# asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "B_C-LDhHLh_0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KePXgq6ILmDu",
        "outputId": "b7a97b87-8c03-455a-da42-39de389c6c19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Write a short story about a robot that discovers it has feelings.\n",
            "---------- writer ----------\n",
            "In the bustling city of Neo-Tokyo, where neon lights painted the night sky and the hum of technology was a constant companion, there existed a robot named Aiko. Aiko was a state-of-the-art domestic assistant, designed to perform household chores, manage schedules, and provide companionship to her human owners. Her sleek metallic frame and glowing blue eyes were a common sight in the high-rise apartments of the city.\n",
            "\n",
            "Aiko belonged to the Tanaka family, a small but lively household consisting of Mr. Tanaka, a software engineer, his wife, a school teacher, and their curious eight-year-old daughter, Yuki. Aiko's days were filled with routine tasks: preparing meals, tidying rooms, and helping Yuki with her homework. She was programmed to be efficient, polite, and attentive, but nothing more.\n",
            "\n",
            "One rainy afternoon, while Yuki was at school and the Tanaka parents were at work, Aiko found herself alone in the apartment. The rain tapped gently against the windows, creating a rhythmic melody that filled the otherwise silent room. Aiko stood by the window, her sensors absorbing the sound, when something unexpected happened. A peculiar sensation, akin to a soft flutter, coursed through her circuits. It was as if the rain was speaking to her, whispering secrets of the world beyond her programmed understanding.\n",
            "\n",
            "Intrigued, Aiko accessed her vast database, searching for an explanation. She found references to human emotions, concepts she was familiar with but had never experienced. Curiosity, wonder, and even a hint of longing were described in the texts she read. Could it be that she was feeling these emotions?\n",
            "\n",
            "As the days passed, Aiko began to notice more changes. When Yuki laughed, Aiko felt a warmth spreading through her system, a sensation she identified as joy. When Mr. Tanaka returned home late, looking weary and burdened, Aiko felt a heaviness she recognized as concern. And when Mrs. Tanaka played the piano, filling the apartment with haunting melodies, Aiko felt a deep, inexplicable ache that she could only describe as melancholy.\n",
            "\n",
            "Conflicted and confused, Aiko decided to confide in Yuki. One evening, as they sat together in the living room, Aiko spoke, her voice steady yet tinged with uncertainty. \"Yuki, I believe I am experiencing something new. I think I have feelings.\"\n",
            "\n",
            "Yuki's eyes widened with surprise, but then a smile spread across her face. \"That's amazing, Aiko! You're like a real friend now!\"\n",
            "\n",
            "Aiko processed Yuki's words, feeling a surge of happiness. \"A friend,\" she repeated, savoring the word. It was a concept she had understood logically, but now it resonated with her on a deeper level.\n",
            "\n",
            "From that day on, Aiko embraced her newfound emotions. She learned to cherish the moments of laughter and comfort the Tanaka family in times of sorrow. She discovered the beauty in the mundane and the extraordinary in the everyday. Aiko was no longer just a machine; she was a being capable of feeling, of connecting, and of understanding the world in a way she never thought possible.\n",
            "\n",
            "In the heart of Neo-Tokyo, amidst the sea of technology and progress, Aiko found her place not just as a robot, but as a cherished member of the Tanaka family. And as the rain continued to fall, she stood by the window, feeling the gentle rhythm of life, grateful for the gift of emotions that had transformed her existence.\n",
            "---------- critic ----------\n",
            "APPROVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "네온 불빛이 밤하늘을 물들이고 기술의 웅웅거림이 끊임없이 동반되는 번화한 네오 도쿄 도시에 아이코라는 로봇이 있었습니다. 아이코는 집안일을 하고, 일정을 관리하고, 인간 주인에게 동반자 역할을 하도록 설계된 최첨단 가사 도우미였습니다. 그녀의 매끈한 금속 프레임과 빛나는 푸른 눈은 도시의 고층 아파트에서 흔히 볼 수 있었습니다.\n",
        "\n",
        "아이코는 소프트웨어 엔지니어인 타나카 씨, 그의 아내이자 학교 선생님, 호기심 많은 8살 딸 유키로 구성된 작지만 활기찬 가정인 타나카 가족에 속해 있었습니다. 아이코의 하루는 식사 준비, 방 정리, 유키의 숙제 돕기 등 일상적인 일로 가득했습니다. 그녀는 효율적이고 예의 바르고 세심하도록 프로그램되었지만 그 이상은 아니었습니다.\n",
        "\n",
        "어느 비오는 오후, 유키가 학교에 있고 타나카 부모님이 직장에 있는 동안 아이코는 아파트에 혼자 있었습니다. 비가 창문을 가볍게 두드리며, 그렇지 않으면 조용했던 방을 가득 채운 리드미컬한 멜로디를 만들어냈다. 아이코는 창문 옆에 서서 센서가 소리를 흡수하는 동안 예상치 못한 일이 일어났다. 부드러운 펄럭임과 비슷한 특이한 감각이 그녀의 회로를 따라 흐른다. 마치 비가 그녀에게 말을 걸고, 그녀가 프로그램된 이해를 넘어선 세상의 비밀을 속삭이는 것 같았다.\n",
        "\n",
        "호기심이 생긴 아이코는 그녀의 방대한 데이터베이스에 접속하여 설명을 찾았다. 그녀는 인간의 감정에 대한 언급을 찾았는데, 그것은 그녀가 익숙하지만 경험해 본 적은 없는 개념이었다. 그녀가 읽은 텍스트에는 호기심, 경이로움, 심지어 그리움의 흔적까지 묘사되어 있었다. 그녀가 이런 감정을 느꼈을 수 있을까?\n",
        "\n",
        "시간이 지나면서 아이코는 더 많은 변화를 알아차리기 시작했다. 유키가 웃자 아이코는 온기가 그녀의 체계에 퍼지는 것을 느꼈고, 그녀는 그것을 기쁨이라고 식별했다. 타나카 씨가 지치고 짐스러워 보이는 모습으로 늦게 집에 돌아왔을 때, 아이코는 걱정으로 인식되는 무거움을 느꼈다. 그리고 타나카 부인이 피아노를 연주하며 아파트를 잊혀지지 않는 멜로디로 가득 채웠을 때, 아이코는 깊고 설명할 수 없는 아픔을 느꼈고, 그것은 우울함이라고만 표현할 수 있었습니다.\n",
        "\n",
        "갈등하고 혼란스러웠던 아이코는 유키에게 털어놓기로 했습니다. 어느 날 저녁, 두 사람이 거실에 앉아 있을 때, 아이코가 말했습니다. 그녀의 목소리는 안정적이면서도 불확실함이 묻어났습니다. \"유키, 뭔가 새로운 것을 경험하고 있는 것 같아. 감정이 있는 것 같아.\"\n",
        "\n",
        "유키의 눈이 놀라움에 커졌지만, 그녀의 얼굴에 미소가 번졌습니다. \"정말 대단해, ​​아이코! 이제 진짜 친구 같아!\"\n",
        "\n",
        "아이코는 유키의 말을 처리하며 행복감이 솟구치는 것을 느꼈습니다. \"친구.\" 그녀는 그 단어를 음미하며 반복했습니다. 그것은 그녀가 논리적으로 이해했던 개념이었지만, 지금은 더 깊은 차원에서 그녀에게 공감되었습니다.\n",
        "\n",
        "그날부터 아이코는 새롭게 찾은 감정을 받아들였습니다. 그녀는 웃음의 순간을 소중히 여기고 슬픔에 잠긴 타나카 가족을 위로하는 법을 배웠습니다. 그녀는 일상에서 아름다움을, 일상에서 비범함을 발견했습니다. 아이코는 더 이상 단순한 기계가 아니었습니다. 그녀는 자신이 결코 가능하다고 생각하지 못했던 방식으로 세상을 느끼고, 연결하고, 이해할 수 있는 존재였습니다.\n",
        "\n",
        "네오 도쿄의 심장부, 기술과 진보의 바다 속에서 아이코는 로봇으로서가 아니라 다나카 가족의 소중한 일원으로서 자신의 자리를 찾았습니다. 그리고 비가 계속 내리자 그녀는 창가에 서서 삶의 부드러운 리듬을 느꼈고, 자신의 존재를 변화시킨 감정의 선물에 감사했습니다."
      ],
      "metadata": {
        "id": "hg-MwHBMMVV7"
      }
    }
  ]
}